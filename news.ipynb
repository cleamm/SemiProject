{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd389d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import sqlite3\n",
    "\n",
    "def makeUrlByDate(search, start_date, end_date):\n",
    "    urls = []\n",
    "    delta = end_date - start_date\n",
    "    for i in range(delta.days + 1):\n",
    "        date = start_date + datetime.timedelta(days=i)\n",
    "        date_str = date.strftime(\"%Y%m%d\")\n",
    "        url = \"https://search.naver.com/search.naver?where=news&query=\" + search + \"&sm=tab_pge&sort=0&photo=0&field=0&reporter_article=&pd=3&ds=\" + date_str + \"&de=\" + date_str + \"&docid=&nso=so%3Ar%2Cp%3Afrom\" + date_str + \"to\" + date_str + \"%2Ca%3A&mynews=0&refresh_start=0&related=0\"\n",
    "        urls.append(url)\n",
    "    return urls\n",
    "\n",
    "# 검색할 키워드 입력\n",
    "search = input(\"검색할 키워드를 입력하세요: \")\n",
    "\n",
    "# 크롤링할 시작 날짜 입력\n",
    "start_date_str = input(\"\\n크롤링할 시작 날짜를 입력하세요 (YYYY-MM-DD 형식): \")\n",
    "start_date = datetime.datetime.strptime(start_date_str, \"%Y-%m-%d\")\n",
    "\n",
    "# 크롤링할 종료 날짜 입력\n",
    "end_date_str = input(\"\\n크롤링할 종료 날짜를 입력하세요 (YYYY-MM-DD 형식): \")\n",
    "end_date = datetime.datetime.strptime(end_date_str, \"%Y-%m-%d\")\n",
    "\n",
    "# naver url 생성\n",
    "urls = makeUrlByDate(search, start_date, end_date)\n",
    "\n",
    "# 뉴스 크롤러 실행\n",
    "news_titles = []\n",
    "news_url = []\n",
    "news_contents = []\n",
    "news_dates = []\n",
    "for i in urls:\n",
    "    url = articles_crawler(i)\n",
    "    news_url.append(url)\n",
    "\n",
    "# 제목, 링크, 내용 1차원 리스트로 꺼내는 함수 생성\n",
    "def makeList(newlist, content):\n",
    "    for i in content:\n",
    "        for j in i:\n",
    "            newlist.append(j)\n",
    "    return newlist\n",
    "\n",
    "# 제목, 링크, 내용 담을 리스트 생성\n",
    "news_url_1 = []\n",
    "\n",
    "# 1차원 리스트로 만들기(내용 제외)\n",
    "makeList(news_url_1, news_url)\n",
    "\n",
    "# NAVER 뉴스만 남기기\n",
    "final_urls = []\n",
    "for i in tqdm(range(len(news_url_1))):\n",
    "    if \"news.naver.com\" in news_url_1[i]:\n",
    "        final_urls.append(news_url_1[i])\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "# 뉴스 내용 크롤링\n",
    "for i in tqdm(final_urls):\n",
    "    # 각 기사 html get하기\n",
    "    news = requests.get(i, headers=headers)\n",
    "    news_html = BeautifulSoup(news.text, \"html.parser\")\n",
    "\n",
    "    # 뉴스 제목 가져오기\n",
    "    title = news_html.select_one(\"#ct > div.media_end_head.go_trans > div.media_end_head_title > h2\")\n",
    "    if title == None:\n",
    "        title = news_html.select_one(\"#content > div.end_ct > div > h2\")\n",
    "\n",
    "    # 뉴스 본문 가져오기\n",
    "    content = news_html.select(\"article#dic_area\")\n",
    "    if content == []:\n",
    "        content = news_html.select(\"#articeBody\")\n",
    "\n",
    "    # 기사 텍스트만 가져오기\n",
    "    # list합치기\n",
    "    content = ''.join(str(content))\n",
    "\n",
    "    # html태그제거 및 텍스트 다듬기\n",
    "    pattern1 = '<[^>]*>'\n",
    "    title = re.sub(pattern=pattern1, repl='', string=str(title))\n",
    "    content = re.sub(pattern=pattern1, repl='', string=content)\n",
    "    pattern2 = \"\"\"[\\n\\n\\n\\n\\n// flash 오류를 우회하기 위한 함수 추가\\nfunction _flash_removeCallback() {}\"\"\"\n",
    "    content = content.replace(pattern2, '')\n",
    "\n",
    "    news_titles.append(title)\n",
    "    news_contents.append(content)\n",
    "\n",
    "    try:\n",
    "        html_date = news_html.select_one(\"div#ct> div.media_end_head.go_trans > div.media_end_head_info.nv_notrans > div.media_end_head_info_datestamp > div > span\")\n",
    "        news_date = html_date.attrs['data-date-time']\n",
    "    except AttributeError:\n",
    "        news_date = news_html.select_one(\"#content > div.end_ct > div > div.article_info > span > em\")\n",
    "        news_date = re.sub(pattern=pattern1, repl='', string=str(news_date))\n",
    "    # 날짜 가져오기\n",
    "    news_dates.append(news_date)\n",
    "\n",
    "print(\"검색된 기사 갯수: 총 \", len(final_urls), '개')\n",
    "print(\"\\n[뉴스 제목]\")\n",
    "print(news_titles)\n",
    "print(\"\\n[뉴스 링크]\")\n",
    "print(final_urls)\n",
    "print(\"\\n[뉴스 내용]\")\n",
    "print(news_contents)\n",
    "\n",
    "print('news_title: ', len(news_titles))\n",
    "print('news_url: ', len(final_urls))\n",
    "print('news_contents: ', len(news_contents))\n",
    "print('news_dates: ', len(news_dates))\n",
    "\n",
    "# 데이터 프레임으로 만들기\n",
    "import pandas as pd\n",
    "\n",
    "# 데이터 프레임 만들기\n",
    "news_df = pd.DataFrame({'date': news_dates, 'title': news_titles, 'link': final_urls, 'content': news_contents})\n",
    "\n",
    "# 중복 행 지우기\n",
    "news_df = news_df.drop_duplicates(keep='first', ignore_index=True)\n",
    "print(\"중복 제거 후 행 개수: \", len(news_df))\n",
    "\n",
    "# 데이터프레임을 JSON 형식으로 변환\n",
    "news_json = news_df.to_json(orient='records', force_ascii=False)\n",
    "\n",
    "# JSON 파일로 저장\n",
    "with open('{}_{}.json'.format(search, now.strftime('%Y%m%d_%H시%M분%S초')), 'w', encoding='utf-8') as json_file:\n",
    "    json_file.write(news_json)\n",
    "\n",
    "# Connect to the SQLite database\n",
    "conn = sqlite3.connect('news.db')\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Create a table to store the news data\n",
    "cursor.execute('''CREATE TABLE IF NOT EXISTS news (\n",
    "                id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "                date TEXT,\n",
    "                title TEXT,\n",
    "                link TEXT,\n",
    "                content TEXT\n",
    "                )''')\n",
    "\n",
    "# Commit changes and close connection\n",
    "conn.commit()\n",
    "\n",
    "# Insert data into the database from the DataFrame\n",
    "for index, row in news_df.iterrows():\n",
    "    cursor.execute('''INSERT INTO news (date, title, link, content) VALUES (?, ?, ?, ?)''', (row['date'], row['title'], row['link'], row['content']))\n",
    "\n",
    "# Commit changes and close connection\n",
    "conn.commit()\n",
    "conn.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a561553d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'SemiProject'...\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/cleamm/SemiProject.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b4c1b93-6505-49fc-935e-660247d194cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import sqlite3\n",
    "\n",
    "def makeUrlByDate(search, start_date, end_date):\n",
    "    urls = []\n",
    "    delta = end_date - start_date\n",
    "    for i in range(delta.days + 1):\n",
    "        date = start_date + datetime.timedelta(days=i)\n",
    "        date_str = date.strftime(\"%Y%m%d\")\n",
    "        url = \"https://search.naver.com/search.naver?where=news&query=\" + search + \"&sm=tab_pge&sort=0&photo=0&field=0&reporter_article=&pd=3&ds=\" + date_str + \"&de=\" + date_str + \"&docid=&nso=so%3Ar%2Cp%3Afrom\" + date_str + \"to\" + date_str + \"%2Ca%3A&mynews=0&refresh_start=0&related=0\"\n",
    "        urls.append(url)\n",
    "    return urls\n",
    "\n",
    "# 검색할 키워드 입력\n",
    "search = input(\"검색할 키워드를 입력하세요: \")\n",
    "\n",
    "# 크롤링할 시작 날짜 입력\n",
    "start_date_str = input(\"\\n크롤링할 시작 날짜를 입력하세요 (YYYY-MM-DD 형식): \")\n",
    "start_date = datetime.datetime.strptime(start_date_str, \"%Y-%m-%d\")\n",
    "\n",
    "# 크롤링할 종료 날짜 입력\n",
    "end_date_str = input(\"\\n크롤링할 종료 날짜를 입력하세요 (YYYY-MM-DD 형식): \")\n",
    "end_date = datetime.datetime.strptime(end_date_str, \"%Y-%m-%d\")\n",
    "\n",
    "# naver url 생성\n",
    "urls = makeUrlByDate(search, start_date, end_date)\n",
    "\n",
    "# 뉴스 크롤러 실행\n",
    "news_titles = []\n",
    "news_url = []\n",
    "news_contents = []\n",
    "news_dates = []\n",
    "for i in urls:\n",
    "    url = articles_crawler(i)\n",
    "    news_url.append(url)\n",
    "\n",
    "# 제목, 링크, 내용 1차원 리스트로 꺼내는 함수 생성\n",
    "def makeList(newlist, content):\n",
    "    for i in content:\n",
    "        for j in i:\n",
    "            newlist.append(j)\n",
    "    return newlist\n",
    "\n",
    "# 제목, 링크, 내용 담을 리스트 생성\n",
    "news_url_1 = []\n",
    "\n",
    "# 1차원 리스트로 만들기(내용 제외)\n",
    "makeList(news_url_1, news_url)\n",
    "\n",
    "# NAVER 뉴스만 남기기\n",
    "final_urls = []\n",
    "for i in tqdm(range(len(news_url_1))):\n",
    "    if \"news.naver.com\" in news_url_1[i]:\n",
    "        final_urls.append(news_url_1[i])\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "# 뉴스 내용 크롤링\n",
    "for i in tqdm(final_urls):\n",
    "    # 각 기사 html get하기\n",
    "    news = requests.get(i, headers=headers)\n",
    "    news_html = BeautifulSoup(news.text, \"html.parser\")\n",
    "\n",
    "    # 뉴스 제목 가져오기\n",
    "    title = news_html.select_one(\"#ct > div.media_end_head.go_trans > div.media_end_head_title > h2\")\n",
    "    if title == None:\n",
    "        title = news_html.select_one(\"#content > div.end_ct > div > h2\")\n",
    "\n",
    "    # 뉴스 본문 가져오기\n",
    "    content = news_html.select(\"article#dic_area\")\n",
    "    if content == []:\n",
    "        content = news_html.select(\"#articeBody\")\n",
    "\n",
    "    # 기사 텍스트만 가져오기\n",
    "    # list합치기\n",
    "    content = ''.join(str(content))\n",
    "\n",
    "    # html태그제거 및 텍스트 다듬기\n",
    "    pattern1 = '<[^>]*>'\n",
    "    title = re.sub(pattern=pattern1, repl='', string=str(title))\n",
    "    content = re.sub(pattern=pattern1, repl='', string=content)\n",
    "    pattern2 = \"\"\"[\\n\\n\\n\\n\\n// flash 오류를 우회하기 위한 함수 추가\\nfunction _flash_removeCallback() {}\"\"\"\n",
    "    content = content.replace(pattern2, '')\n",
    "\n",
    "    news_titles.append(title)\n",
    "    news_contents.append(content)\n",
    "\n",
    "    try:\n",
    "        html_date = news_html.select_one(\"div#ct> div.media_end_head.go_trans > div.media_end_head_info.nv_notrans > div.media_end_head_info_datestamp > div > span\")\n",
    "        news_date = html_date.attrs['data-date-time']\n",
    "    except AttributeError:\n",
    "        news_date = news_html.select_one(\"#content > div.end_ct > div > div.article_info > span > em\")\n",
    "        news_date = re.sub(pattern=pattern1, repl='', string=str(news_date))\n",
    "    # 날짜 가져오기\n",
    "    news_dates.append(news_date)\n",
    "\n",
    "print(\"검색된 기사 갯수: 총 \", len(final_urls), '개')\n",
    "print(\"\\n[뉴스 제목]\")\n",
    "print(news_titles)\n",
    "print(\"\\n[뉴스 링크]\")\n",
    "print(final_urls)\n",
    "print(\"\\n[뉴스 내용]\")\n",
    "print(news_contents)\n",
    "\n",
    "print('news_title: ', len(news_titles))\n",
    "print('news_url: ', len(final_urls))\n",
    "print('news_contents: ', len(news_contents))\n",
    "print('news_dates: ', len(news_dates))\n",
    "\n",
    "# 데이터 프레임으로 만들기\n",
    "import pandas as pd\n",
    "\n",
    "# 데이터 프레임 만들기\n",
    "news_df = pd.DataFrame({'date': news_dates, 'title': news_titles, 'link': final_urls, 'content': news_contents})\n",
    "\n",
    "# 중복 행 지우기\n",
    "news_df = news_df.drop_duplicates(keep='first', ignore_index=True)\n",
    "print(\"중복 제거 후 행 개수: \", len(news_df))\n",
    "\n",
    "# 데이터프레임을 JSON 형식으로 변환\n",
    "news_json = news_df.to_json(orient='records', force_ascii=False)\n",
    "\n",
    "# JSON 파일로 저장\n",
    "with open('{}_{}.json'.format(search, now.strftime('%Y%m%d_%H시%M분%S초')), 'w', encoding='utf-8') as json_file:\n",
    "    json_file.write(news_json)\n",
    "\n",
    "# Connect to the SQLite database\n",
    "conn = sqlite3.connect('news.db')\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Create a table to store the news data\n",
    "cursor.execute('''CREATE TABLE IF NOT EXISTS news (\n",
    "                id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "                date TEXT,\n",
    "                title TEXT,\n",
    "                link TEXT,\n",
    "                content TEXT\n",
    "                )''')\n",
    "\n",
    "# Commit changes and close connection\n",
    "conn.commit()\n",
    "\n",
    "# Insert data into the database from the DataFrame\n",
    "for index, row in news_df.iterrows():\n",
    "    cursor.execute('''INSERT INTO news (date, title, link, content) VALUES (?, ?, ?, ?)''', (row['date'], row['title'], row['link'], row['content']))\n",
    "\n",
    "# Commit changes and close connection\n",
    "conn.commit()\n",
    "conn.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
